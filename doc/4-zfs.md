<!--
    This Source Code Form is subject to the terms of the Mozilla Public
    License, v. 2.0. If a copy of the MPL was not distributed with this
    file, You can obtain one at http://mozilla.org/MPL/2.0/.
-->

<!--
    Copyright 2020 Joyent, Inc
-->

# ZFS integration

To promote consistency and reuse across compute node types, ZFS is used on Linux
compute nodes.  The argument for consistency with the Triton ecosystem should
not be taken as an indication that there is a compromise: ZFS is a great file
system that is well-suited for the task at hand.

This document describes how ZFS is used and limitations it imposes.

## Distribution of ZFS kernel modules

Binary distribution of ZFS kernel modules for Linux (particularly by those
distributing the matching Linux kernel?) is not allowed due to license
incompatibilities.  For this reason, any platform images that are distributed
must be distributed without ZFS kernel modules.

This imposes the following requirements:

- There needs to be a pain-free way to each Triton user (e.g. a company) to
  build the modules.
- The modules need to be incorporated into the (squashfs?) image or
  automatically installed at boot time in some other way.

**NOTICE:** The images generated by [debian-live](../tools/debian-live) must not
be distributed outside of the organization that generates them.  See the [image
build instructions](2-platform-image-construction.md) for instructions on how to
use this repository to build your own image during these early development days.


## Dataset hierarchy

### system pool name

In SmartOS, it made sense to call the pool `zones`.  Here, not so much.  Better
names may be:

- system:  Triton expects to use a system pool.  Naming a pool that is somewhat
  attractive, but there's nothing saying that a pool with this name would be the
  system pool.  Maybe that would be confusing.  Also, if this pool is imported
  while running SmartOS, `/system` is already in use.
- sys: Similar to the previous option, but that conflicts with `/sys` on Linux.
- triton: Unlikely to be in use by anything else and is perfect until the next
  rebranding.

For now the default pool name shall be `triton`.

### Mounts over root fs directories

There are some file systems that should be mounted over well-known directories
in the root file system so that systemd and perhaps other components can make
use of them.  For instance, persistent network configuration is likely to reside
in `/etc/systemd/network`.

The current plan of record is to create a hierarchy under `triton/system` such
that file systems under that are automatically mounted over directories in the
root file system.

The top-level is created as:

```
# zfs create -o canmount=noauto -o mountpoint=/ triton/system
```

Now, datasets under that dataset automaticlaly get the right mountpoint.  For
example, this will creaete the file system that gets mounted at `/opt`.

```
# zfs create triton/system/opt
```

When needing to create a spot in the directory hierarchy that shouldn't get
mounted over the root file system, just set `canmount=noauto`.

```
# zfs create -o canmount=noauto triton/system/etc
# zfs create -o canmount=noauto triton/system/etc/systemd
# zfs create triton/system/etc/systemd/network

# zfs create -o canmount=noauto triton/system/var
# zfs create triton/system/var/imgadm
# zfs create triton/system/var/sshd
```

### Standard datasets and mount points

| Mountpoint           | ZFS Filesystem                 | Notes                |
|----------------------|--------------------------------|----------------------|
| /etc/systemd/network | triton/platform/etc/systemd/network | [systemd.network](https://systemd.network/systemd.network.html) |
| /etc/sysusers.d      | triton/platform/etc/sysusers.d | [systemd.sysusers.d](https://www.freedesktop.org/software/systemd/man/sysusers.d.html) |
| /home                | triton/platform/home           | Useful for developers home dirs, maybe useful for customers requiring non-root system users. Use in conjunction with `/etc/sysusers.d`. |
| /opt                 | triton/platform/opt            | Likely location of installed agents. |
| /root                | triton/platform/root           | Persistent root dir helpful for `authorized_keys` and such. |
| /triton              | triton                         | Default mount point for pool.  Maybe we shouldn't mount it? |
| /var/imgadm          | triton/platform/var/imgadm     | Image manifests and such. |
| /var/lib/machines/\* | triton/:instance\_uuid         | /var/lib/machines helps with [machinectl] integration |
| /var/ssh             | triton/platform/var/ssh        | ssh keys |

## Boot integration

debian-live images boot only from ISO or USB.  Work has not yet begun on making
it boot from iPXE or other mechanisms suggested by [RFD
176](https://github.com/joyent/rfd/blob/master/rfd/0176/README.md).  Notably,
this will require additional work to make it so that `filesystem.squashfs` is
obtained automatically from a separate file or it or its content is included in
`initrd`.

When the tools in this repository are used to build a debian-live image, the
non-dkms version of ZFS is installed in `live/filesystem.squashfs` root file
system.  This means that systemd services found in the squashfs archive are
responsible for importing the pool and mounting file systems.  Since there are
some systemd services that are defined in files defined or altered by files
that exist in the system zpool, there are *hook scripts* and *boot scripts*, as
described in `initramfs-tools(7)`, that ensure that the persistent data stored
in ZFS is mounted before systemd starts.  In particular:

- [sethostid](../src/sethostid) is run to set the hostid to a hash of the
  system's UUID.  This is the same system UUID that is used by Triton.
- The [triton](../proto/usr/share/initramfs-tools/hooks/triton) *hook script*
  ensures that `sethostid`, `dmidecode`, and any libraries they require are in
  `initrd`.
- The [syspool](../proto/usr/share/initramfs-tools/scripts/live-bottom/syspool)
  *boot script* runs at boot to find the system pool, import it, and mount the
  ZFS file systems that are needed early in boot.  Services that can start later
  (e.g. Triton agents) should include `After=zfs.target` so they start after the
  systemd mounts the remaining datasets.


## Future directions

The following describe ideas, not decisions.

### Dataset delegation

It is expected that work will be required to make `delegated_dataset` work.  If
this is ever to host headnode services, build images, etc., that will likely be
important.

This is likely to be a significant amount of work, potentially adding a new
namespace for zfs.

### ZFS security

We need to be sure that container users are not able to muck with zfs or see
datasets not mounted in the container or delegated to it.  It may be that we
just need to be sure that `/dev/zfs` does not appear in containers and that
containers are not able to create new device nodes.
