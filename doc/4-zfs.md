<!--
    This Source Code Form is subject to the terms of the Mozilla Public
    License, v. 2.0. If a copy of the MPL was not distributed with this
    file, You can obtain one at http://mozilla.org/MPL/2.0/.
-->

<!--
    Copyright 2020 Joyent, Inc
-->

# ZFS integration

To promote consistency and reuse across compute node types, ZFS is used on Linux
compute nodes.  The argument for consistency with the Triton ecosystem should
not be taken as an indication that there is a compromise: ZFS is a great file
system that is well-suited for the task at hand.

This document describes how ZFS is used and limitations it imposes.

## Distribution of ZFS kernel modules

Binary distribution of ZFS kernel modules for Linux (particularly by those
distributing the matching Linux kernel?) is not allowed due to license
incompatibilities.  For this reason, any platform images that are distributed
must be distributed without ZFS kernel modules.

This imposes the following requirements:

- There needs to be a pain-free way to each Triton user (e.g. a company) to
  build the modules.
- The modules need to be incorporated into the (squashfs?) image or
  automatically installed at boot time in some other way.

**NOTICE:** The images generated by [debian-live](../tools/debian-live) must not
be distributed outside of the organization that generates them.  See the [image
build instructions](2-platform-image-construction.md) for instructions on how to
use this repository to build your own image during these early development days.


## Dataset hierarchy

### system pool name

In SmartOS, it made sense to call the pool `zones`.  Here, not so much.  Better
names may be:

- system:  Triton expects to use a system pool.  Naming a pool that is somewhat
  attractive, but there's nothing saying that a pool with this name would be the
  system pool.  Maybe that would be confusing.  Also, if this pool is imported
  while running SmartOS, `/system` is already in use.
- sys: Similar to the previous option, but that conflicts with `/sys` on Linux.
- triton: Unlikely to be in use by anything else and is perfect until the next
  rebranding.

For now the default pool name shall be `triton`.

### Mounts over root fs directories

There are some file systems that should be mounted over well-known directories
in the root file system so that systemd and perhaps other components can make
use of them.  For instance, persistent network configuration is likely to reside
in `/etc/systemd/network`.

The current plan of record is to create a hierarchy under `triton/system` such
that file systems under that are automatically mounted over directories in the
root file system.

The top-level is created as:

```
# zfs create -o canmount=noauto -o mountpoint=/ triton/system
```

Now, datasets under that dataset automaticlaly get the right mountpoint.  For
example, this will creaete the file system that gets mounted at `/opt`.

```
# zfs create triton/system/opt
```

When needing to create a spot in the directory hierarchy that shouldn't get
mounted over the root file system, just set `canmount=noauto`.

```
# zfs create -o canmount=noauto triton/system/etc
# zfs create -o canmount=noauto triton/system/etc/systemd
# zfs create triton/system/etc/systemd/network

# zfs create -o canmount=noauto triton/system/var
# zfs create triton/system/var/imgadm
# zfs create triton/system/var/sshd
```


## Boot integration

debian-live images boot only from ISO or USB.  Work has not yet begun on making
it boot from iPXE or other mechanisms suggested by [RFD
176](https://github.com/joyent/rfd/blob/master/rfd/0176/README.md).  Notably,
this will require additional work to make it so that `filesystem.squashfs` is
obtained automatically from a separate file or it or its content is included in
`initrd`.

When the tools in this repository are used to build a debian-live image, the
non-dkms version of ZFS is installed in `live/filesystem.squashfs` root file
system.  This means that systemd services found in the squashfs archive are
responsible for importing the pool and mounting file systems.  Since there are
some systemd services that are defined in files defined or altered by files
that exist in the system zpool, there are hooks in the platform image to trigger
various systemd reload and restart actions after the pool is imported.  See
[*Platform image services*](3-systemd-integration.md#platform-image-services)
for details.


## Future directions

The following describe ideas, not decisions.

### Import system pool before systemd starts

The current boot order is along the lines of:

* grub
  * load `vmlinuz` and `initrd`
  * boot with `boot=live` on the command line
* While running from `initrd`
  * `init` is a shell script (`/init`)
  * The `boot=live` kernel command line argument triggers use of
    [linux-live](https://www.linux-live.org/) to find `live/filesystem.squashfs`
    on the boot medium and mount it on an altnerate root
  * Various mounts are performed
  * root is pivoted to the squashfs root
  * systemd is started
  * all of the stuff that has to happen early happens
  * import the system ZFS pool
  * mount ZFS file systems
  * systemd and various services are nudged to learn about the things that
    popped up in the just-mounted directories

Many of the gyrations described in
[*Platform image services*](3-systemd-integration.md#platform-image-services)
could be avoided if the process was changed to be:

* grub
  * load `vmlinuz` and `initrd`
  * boot with `boot=live` on the command line
* While running from `initrd`
  * `init` is a shell script (`/init`)
  * The `boot=live` kernel command line argument triggers use of
    [linux-live](https://www.linux-live.org/) to find `live/filesystem.squashfs`
    on the boot medium and mount it on an altnerate root
  * **import the system pool**
  * **mount zfs file systems that are important for system boot**
  * Various mounts are performed
  * root is pivoted to the squashfs root
  * systemd is started
  * all of the stuff that has to happen early happens
  * ~~import the system ZFS pool~~
  * mount **the remaining** ZFS file systems
  * ~~systemd and various services are nudged to learn about the things that
    popped up in the just-mounted directories~~

### Dataset delegation

It is expected that work will be required to make `delegated_dataset` work.  If
this is ever to host headnode services, build images, etc., that will likely be
important.

This is likely to be a significant amount of work, potentially adding a new
namespace for zfs.

### ZFS security

We need to be sure that container users are not able to muck with zfs or see
datasets not mounted in the container or delegated to it..
